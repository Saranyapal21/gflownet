{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69883459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (10,), (11,), (12,), (13,), (14,), (15,), (16,), (17,), (18,), (19,), (20,), (21,), (22,), (23,), (24,), (25,), (26,), (-1,)]\n",
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from gflownet.envs.scrabble import Scrabble\n",
    "env = Scrabble()\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6165049",
   "metadata": {},
   "source": [
    "## Testing GFlowNet on Frozen-Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fbe56",
   "metadata": {},
   "source": [
    "### My Code:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5a1c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from gymnasium import ActionWrapper, RewardWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e342e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4526da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedStep(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.grid_size = int(np.sqrt(env.observation_space.n))\n",
    "        self.goal_state = self._find_goal_state()\n",
    "        self.goal_pos = self._state_to_pos(self.goal_state)\n",
    "\n",
    "    def _state_to_pos(self, state):\n",
    "        return (state // self.grid_size, state % self.grid_size)\n",
    "\n",
    "    def _find_goal_state(self):\n",
    "        desc = self.env.unwrapped.desc  # shape (4, 4), values like b'S', b'F', b'G'\n",
    "        for r in range(desc.shape[0]):\n",
    "            for c in range(desc.shape[1]):\n",
    "                if desc[r][c] == b'G':\n",
    "                    return r * self.grid_size + c\n",
    "        raise ValueError(\"Goal state not found\")\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.env.unwrapped.s\n",
    "        prev_pos = self._state_to_pos(prev_state)\n",
    "        prev_dist = self._manhattan(prev_pos, self.goal_pos)\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        next_pos = self._state_to_pos(next_state)\n",
    "        next_dist = self._manhattan(next_pos, self.goal_pos)\n",
    "\n",
    "        if reward == 1.0:\n",
    "            return next_state, 1.0, terminated, truncated, info\n",
    "        elif next_dist < prev_dist:\n",
    "            return next_state, 0.1, terminated, truncated, info\n",
    "        else:\n",
    "            return next_state, 0.0, terminated, truncated, info\n",
    "\n",
    "    def _manhattan(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d9e9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedStep(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30830793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT (0, 0.0, False, False, {'prob': 1.0})\n",
      "LEFT (1, 0.1, False, False, {'prob': 1.0})\n",
      "LEFT (2, 0.1, False, False, {'prob': 1.0})\n",
      "LEFT (3, 0.1, False, False, {'prob': 1.0})\n",
      "DOWN (7, 0.1, True, False, {'prob': 1.0})\n",
      "RIGHT (7, 0.0, True, False, {'prob': 1.0})\n",
      "DOWN (7, 0.0, True, False, {'prob': 1.0})\n",
      "UP (7, 0.0, True, False, {'prob': 1.0})\n",
      "DOWN (7, 0.0, True, False, {'prob': 1.0})\n",
      "DOWN (7, 0.0, True, False, {'prob': 1.0})\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    act_dict = {0:'RIGHT', 1:'DOWN', 2:'LEFT', 3:'UP'}\n",
    "    print(act_dict[action], env.step(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca8407",
   "metadata": {},
   "source": [
    "### Simplified Version:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "578a8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class SimpleFrozenLake:\n",
    "    def __init__(self, map_name=\"4x4\", is_slippery=False):\n",
    "        # Create the FrozenLake environment\n",
    "        self.env = gym.make(\"FrozenLake-v1\", map_name=map_name, is_slippery=is_slippery)\n",
    "        self.eos = 4  # Special \"End Of Sequence\" action\n",
    "        self.action_space = [0, 1, 2, 3, self.eos]  # 4 directions + EOS\n",
    "        self.state = None\n",
    "        self.done = False\n",
    "        self.n_actions = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.state = int(self.state)\n",
    "        self.done = False\n",
    "        self.n_actions = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.state, action, True\n",
    "\n",
    "        if action == self.eos:\n",
    "            self.done = True\n",
    "            self.n_actions += 1\n",
    "            return self.state, action, True\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        self.state = int(next_state)\n",
    "        self.n_actions += 1\n",
    "        self.done = terminated or truncated\n",
    "        return self.state, action, True\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.action_space\n",
    "\n",
    "    def state_to_coordinates(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.state\n",
    "        row, col = divmod(state, 4)\n",
    "        return f\"({row}, {col})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a118711",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleFrozenLake()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b5117",
   "metadata": {},
   "source": [
    "### GPT Code:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dda5dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from gflownet.envs.base import GFlowNetEnv\n",
    "from gflownet.utils.common import tlong, tfloat\n",
    "\n",
    "class FrozenLake(GFlowNetEnv):\n",
    "    def __init__(self,\n",
    "                 map_name: str = \"4x4\",\n",
    "                 is_slippery: bool = False,\n",
    "                 **kwargs):\n",
    "        # underlying Gym env\n",
    "        self.env = gym.make(\"FrozenLake-v1\",\n",
    "                            map_name=map_name,\n",
    "                            is_slippery=is_slippery)\n",
    "        # EOS pseudo-action\n",
    "        self.eos = 4\n",
    "        # call parent ctor (sets device, logger helpers, etc.)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    # ───────────── required API ───────────── #\n",
    "    def get_action_space(self):\n",
    "        # 4 navigation moves + EOS\n",
    "        return [0, 1, 2, 3, self.eos]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.state = int(self.state)          # make it a plain int\n",
    "        self.done = False\n",
    "        self.n_actions = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action: Tuple[int], skip_mask_check: bool = False) -> Tuple[int, int, bool]:\n",
    "        do_step, self.state, action = self._pre_step(\n",
    "            action, skip_mask_check or self.skip_mask_check)\n",
    "        if not do_step:                              # mask said “invalid”\n",
    "            return self.state, action, False\n",
    "\n",
    "        # EOS action: mark done but leave state unchanged\n",
    "        if action == self.eos:\n",
    "            self.done = True\n",
    "            self.n_actions += 1\n",
    "            return self.state, action, True\n",
    "\n",
    "        # Forward step in Gym env\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        self.state = int(next_state)\n",
    "        self.n_actions += 1\n",
    "        self.done = terminated or truncated\n",
    "        # Auto-inject EOS once the lake episode finishes\n",
    "        if self.done:\n",
    "            action = self.eos\n",
    "        return self.state, action, True\n",
    "\n",
    "    # ───────────── masks ───────────── #\n",
    "    def get_mask_invalid_actions_forward(self,\n",
    "                                         state: Optional[int] = None,\n",
    "                                         done: Optional[bool] = None):\n",
    "        \"\"\"Allow EOS only after termination.\"\"\"\n",
    "        done = self._get_done(done)\n",
    "        if done:                   # only EOS valid\n",
    "            return [a != self.eos for a in self.action_space]\n",
    "        else:                      # navigation moves valid, EOS invalid\n",
    "            return [a == self.eos for a in self.action_space]\n",
    "\n",
    "    def get_parents(self, state=None, done=None, action=None):\n",
    "        # For Trajectory-Balance objectives we can get by with a dummy\n",
    "        # implementation that says “parent is self + EOS”.\n",
    "        return [state], [self.eos]\n",
    "\n",
    "    # ───────────── encodings ───────────── #\n",
    "    def states2policy(self,\n",
    "                      states: Union[List[int],\n",
    "                                    TensorType[\"batch\"]]):     # noqa: F821\n",
    "        \"\"\"One-hot encode 16 discrete tiles.\"\"\"\n",
    "        states = tlong(states, device=self.device)\n",
    "        n = states.shape[0]\n",
    "        out = torch.zeros(n, 16, dtype=self.float, device=self.device)\n",
    "        out[torch.arange(n, device=self.device), states] = 1.\n",
    "        return out\n",
    "\n",
    "    def states2proxy(self, states):\n",
    "        # No extra features – just reuse one-hot\n",
    "        return self.states2policy(states)\n",
    "\n",
    "    def state2readable(self, state=None, alphabet=None):\n",
    "        s = self._get_state(state)\n",
    "        r, c = divmod(s, 4)\n",
    "        return f\"({r},{c})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a6ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
