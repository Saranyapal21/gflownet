{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69883459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (10,), (11,), (12,), (13,), (14,), (15,), (16,), (17,), (18,), (19,), (20,), (21,), (22,), (23,), (24,), (25,), (26,), (-1,)]\n",
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from gflownet.envs.scrabble import Scrabble\n",
    "env = Scrabble()\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6165049",
   "metadata": {},
   "source": [
    "## Testing GFlowNet on Frozen-Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fbe56",
   "metadata": {},
   "source": [
    "### My Code:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5a1c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from gymnasium import ActionWrapper, RewardWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e342e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4526da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedStep(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.grid_size = int(np.sqrt(env.observation_space.n))\n",
    "        self.goal_state = self._find_goal_state()\n",
    "        self.goal_pos = self._state_to_pos(self.goal_state)\n",
    "\n",
    "    def _state_to_pos(self, state):\n",
    "        return (state // self.grid_size, state % self.grid_size)\n",
    "\n",
    "    def _find_goal_state(self):\n",
    "        desc = self.env.unwrapped.desc  # shape (4, 4), values like b'S', b'F', b'G'\n",
    "        for r in range(desc.shape[0]):\n",
    "            for c in range(desc.shape[1]):\n",
    "                if desc[r][c] == b'G':\n",
    "                    return r * self.grid_size + c\n",
    "        raise ValueError(\"Goal state not found\")\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.env.unwrapped.s\n",
    "        prev_pos = self._state_to_pos(prev_state)\n",
    "        prev_dist = self._manhattan(prev_pos, self.goal_pos)\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        next_pos = self._state_to_pos(next_state)\n",
    "        next_dist = self._manhattan(next_pos, self.goal_pos)\n",
    "\n",
    "        if reward == 1.0:\n",
    "            return next_state, 1.0, terminated, truncated, info\n",
    "        elif next_dist < prev_dist:\n",
    "            return next_state, 0.1, terminated, truncated, info\n",
    "        else:\n",
    "            return next_state, 0.0, terminated, truncated, info\n",
    "\n",
    "    def _manhattan(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d9e9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedStep(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30830793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT (0, 0.0, False, False, {'prob': 1.0})\n",
      "LEFT (1, 0.1, False, False, {'prob': 1.0})\n",
      "LEFT (2, 0.1, False, False, {'prob': 1.0})\n",
      "LEFT (3, 0.1, False, False, {'prob': 1.0})\n",
      "DOWN (7, 0.1, True, False, {'prob': 1.0})\n",
      "RIGHT (7, 0.0, True, False, {'prob': 1.0})\n",
      "DOWN (7, 0.0, True, False, {'prob': 1.0})\n",
      "UP (7, 0.0, True, False, {'prob': 1.0})\n",
      "DOWN (7, 0.0, True, False, {'prob': 1.0})\n",
      "DOWN (7, 0.0, True, False, {'prob': 1.0})\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    act_dict = {0:'RIGHT', 1:'DOWN', 2:'LEFT', 3:'UP'}\n",
    "    print(act_dict[action], env.step(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76782b",
   "metadata": {},
   "source": [
    "## Implementation of BPE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with 'Ġ'\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including 'Ġ' if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # Extend unique_chars with characters from processed_text that are not already included\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # Optionally, ensure 'Ġ' is included if it is relevant to your text processing\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # Now create the vocab and inverse vocab dictionaries\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:  # No more pairs to merge. Stopping training.\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # loaded_vocab maps token_str to token_id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}  # token_id: token_str\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}  # token_str: token_id\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            # Skip header line if present\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for rank, line in enumerate(lines):\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) != 2:\n",
    "                    print(f\"Line {rank+1} has more than 2 entries: {line.strip()}\")\n",
    "                    continue\n",
    "                token1, token2 = pair\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    token_id1 = self.inverse_vocab[token1]\n",
    "                    token_id2 = self.inverse_vocab[token2]\n",
    "                    merged_token = token1 + token2\n",
    "                    if merged_token in self.inverse_vocab:\n",
    "                        merged_token_id = self.inverse_vocab[merged_token]\n",
    "                        self.bpe_merges[(token_id1, token_id2)] = merged_token_id\n",
    "                        # print(f\"Loaded merge: '{token1}' + '{token2}' -> '{merged_token}' (ID: {merged_token_id})\")\n",
    "                    else:\n",
    "                        print(f\"Merged token '{merged_token}' not found in vocab. Skipping.\")\n",
    "                else:\n",
    "                    print(f\"Skipping pair {pair} as one of the tokens is not in the vocabulary.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Split text into tokens, keeping newlines intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  # Ensure '\\n' is treated as a separate token\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # Add 'Ġ' to words that follow a space or newline\n",
    "            else:\n",
    "                tokens.append(word)  # Handle first word or standalone '\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token is contained in the vocabulary as is\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # Attempt to handle subword tokenization via BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    # Uncomment for educational purposes:\n",
    "                    # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # Replace 'Ġ' with a space\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump({k: v for k, v in self.vocab.items()}, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge['pair'])\n",
    "                new_id = merge['new_id']\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
